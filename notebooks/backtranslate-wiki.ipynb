{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"LazarusNLP/wikipedia_id_20230520\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda example: 150 < len(example['text']) < 500, num_proc=30).remove_columns([\"id\", \"url\", \"title\"])\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.select(range(1_000_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctranslate2\n",
    "import transformers\n",
    "\n",
    "translator = ctranslate2.Translator(\"opus-mt-id-en\", device=\"cuda\", compute_type=\"bfloat16\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-id-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda example: {\"text_tokenized\": tokenizer.convert_ids_to_tokens(tokenizer.encode(example[\"text\"]))}, num_proc=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(examples):\n",
    "    results = translator.translate_batch(examples['text_tokenized'])\n",
    "    examples[\"text_en\"] = [tokenizer.decode(tokenizer.convert_tokens_to_ids(result.hypotheses[0])) for result in results]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function translate_batch at 0x7fa2e1c63760> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 1000000/1000000 [20:43<00:00, 804.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(translate_batch, batched=True, batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = ctranslate2.Translator(\"opus-mt-en-id\", device=\"cuda\", compute_type=\"bfloat16\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=30): 100%|██████████| 1000000/1000000 [00:09<00:00, 104776.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda example: {\"text_en_tokenized\": tokenizer.convert_ids_to_tokens(tokenizer.encode(example[\"text_en\"]))}, num_proc=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(examples):\n",
    "    results = translator.translate_batch(examples['text_en_tokenized'])\n",
    "    examples[\"text_bt\"] = [tokenizer.decode(tokenizer.convert_tokens_to_ids(result.hypotheses[0])) for result in results]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000000/1000000 [18:39<00:00, 892.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(translate_batch, batched=True, batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"text_tokenized\", \"text_en_tokenized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Setiap tahunnya, Orange Grove Boulevard menjadi panggung Turnamen Parade Mawar. Pagi-pagi buta sebelum fajar, kendaraan-kendaraan peserta dari segala ukuran dan bentuk dapat dilihat diparkir memanjang sepanjang boulevard ini sementara para awak relawannya bergegas memberikan sentuhannya yang terakhir.',\n",
       " 'text_en': \"Every year, Orange Grove Boulevard becomes a Rose Parade tournament stage early morning before dawn, the participants' vehicles of all sizes and shapes can be seen parked long-long through this boulevard while the volunteers rush to give their last touch.\",\n",
       " 'text_bt': 'Setiap tahun, Orange Grove Boulevard menjadi panggung turnamen Parade Rose pagi-pagi sebelum fajar, kendaraan peserta dari semua ukuran dan bentuk dapat dilihat diparkir panjang melalui boulevard ini sementara relawan bergegas untuk memberikan sentuhan terakhir mereka.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:01<00:00, 427.19ba/s]\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/lfs.py:310: UserWarning: hf_transfer is enabled but does not support uploading from bytes or BinaryIO, falling back to regular upload\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.45s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 500/500 [00:01<00:00, 431.51ba/s]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.68s/it]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:20<00:00, 10.18s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset.push_to_hub(\"LazarusNLP/wikipedia_id_backtranslated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
