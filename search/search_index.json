{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Indonesian Sentence Embeddings","text":"<p>Inspired by Thai Sentence Vector Benchmark, we decided to embark on the journey of training Indonesian sentence embedding models!</p> <p> </p>"},{"location":"#evaluation","title":"Evaluation","text":""},{"location":"#semantic-textual-similarity","title":"Semantic Textual Similarity","text":"<p>We followed approached done in the Thai Sentence Vector Benchmark project and translated the STS-B dev and test set to Indonesian via Google Translate API. This dataset will be used to evaluate our model's Spearman correlation score on the translated test set.</p> <p>You can find the translated dataset on \ud83e\udd17 HuggingFace Hub.</p> <p>Further, we will similarly be evaluating our models on the SemRel2024 dataset which contains human-annotated, Indonesian semantic textual relatedness (STR) data. The dataset consists of two splits: <code>dev</code> and <code>test</code>. We will be evaluating our models' Spearman correlation score on both splits.</p>"},{"location":"#retrieval","title":"Retrieval","text":"<p>To evaluate our models' capability to perform retrieval tasks, we evaluate them on Indonesian subsets of MIRACL and TyDiQA datasets. In both datasets, the model's ability to retrieve relevant documents given a query is tested. We employ R@1 (top-1 accuracy), MRR@10, and nDCG@10 metrics to measure our model's performance.</p>"},{"location":"#classification","title":"Classification","text":"<p>For text classification, we will be doing emotion classification and sentiment analysis on the EmoT and SmSA subsets of IndoNLU, respectively. To do so, we will be doing the same approach as Thai Sentence Vector Benchmark and simply fit a Linear SVC on sentence representations of our texts with their corresponding labels. Thus, unlike conventional fine-tuning method where the backbone model is also updated, the Sentence Transformer stays frozen in our case; with only the classification head being trained.</p> <p>Further, we will evaluate our models using the official MTEB code that contains two Indonesian classification subtasks: <code>MassiveIntentClassification (id)</code> and <code>MassiveScenarioClassification (id)</code>.</p>"},{"location":"#pair-classification","title":"Pair Classification","text":"<p>We followed MTEB's PairClassification evaluation procedure for pair classification. Specifically for zero-shot natural language inference tasks, all neutral pairs are dropped, while contradictions and entailments are re-mapped as <code>0</code>s and <code>1</code>s. The maximum average precision (AP) score is found by finding the best threshold value.</p> <p>We leverage the IndoNLI dataset's two test subsets: <code>test_lay</code> and <code>test_expert</code>.</p>"},{"location":"#methods","title":"Methods","text":""},{"location":"#unsupervised-simcse","title":"(Unsupervised) SimCSE","text":"<p>We followed SimCSE: Simple Contrastive Learning of Sentence Embeddings and trained a sentence embedding model in an unsupervised fashion. Unsupervised SimCSE allows us to leverage an unsupervised corpus -- which are plenty -- and with different dropout masks in the encoder, contrastively learn sentence representations. This is parallel with the situation that there is a lack of supervised Indonesian sentence similarity datasets, hence SimCSE is a natural first move into this field. We used the Sentence Transformer implementation of SimCSE.</p>"},{"location":"#congen","title":"ConGen","text":"<p>Like SimCSE, ConGen: Unsupervised Control and Generalization Distillation For Sentence Representation is another unsupervised technique to train a sentence embedding model. Since it is in-part a distillation method, ConGen relies on a teacher model which will then be distilled to a student model. The original paper proposes back-translation as the best data augmentation technique. However, due to the lack of resources, we implemented word deletion, which was found to be on-par with back-translation despite being trivial. We used the official ConGen implementation which was written on top of the Sentence Transformers library.</p>"},{"location":"#sct","title":"SCT","text":"<p>SCT: An Efficient Self-Supervised Cross-View Training For Sentence Embedding is another unsupervised technique to train a sentence embedding model. It is very similar to ConGen in its knowledge distillation methodology, but also supports self-supervised training procedure without a teacher model. The original paper proposes back-translation as its data augmentation technique, but we implemented single-word deletion and found it to perform better than our backtranslated corpus. We used the official SCT implementation which was written on top of the Sentence Transformers library.</p>"},{"location":"#pretrained-models","title":"Pretrained Models","text":"Model #params Base/Student Model Teacher Model Train Dataset Supervised SimCSE-IndoBERT Base 125M IndoBERT Base N/A Wikipedia ConGen-IndoBERT Lite Base 12M IndoBERT Lite Base paraphrase-multilingual-mpnet-base-v2 Wikipedia ConGen-IndoBERT Base 125M IndoBERT Base paraphrase-multilingual-mpnet-base-v2 Wikipedia ConGen-SimCSE-IndoBERT Base 125M SimCSE-IndoBERT Base paraphrase-multilingual-mpnet-base-v2 Wikipedia ConGen-Indo-e5 Small 118M multilingual-e5-small paraphrase-multilingual-mpnet-base-v2 Wikipedia SCT-IndoBERT Base 125M IndoBERT Base paraphrase-multilingual-mpnet-base-v2 Wikipedia all-IndoBERT Base 125M IndoBERT Base N/A See: README \u2705 all-IndoBERT Base-v2 125M IndoBERT Base N/A See: README \u2705 all-IndoBERT Base-v4 125M IndoBERT Base N/A See: README \u2705 all-NusaBERT Base-v4 111M NusaBERT Base N/A See: README \u2705 all-NusaBERT Large-v4 337M NusaBERT Large N/A See: README \u2705 all-Indo-e5 Small-v2 118M multilingual-e5-small N/A See: README \u2705 all-Indo-e5 Small-v3 118M multilingual-e5-small N/A See: README \u2705 all-Indo-e5 Small-v4 118M multilingual-e5-small N/A See: README \u2705 distiluse-base-multilingual-cased-v2 134M DistilBERT Base Multilingual mUSE See: SBERT \u2705 paraphrase-multilingual-mpnet-base-v2 125M XLM-RoBERTa Base paraphrase-mpnet-base-v2 See: SBERT \u2705 multilingual-e5-small 118M Multilingual-MiniLM-L12-H384 See: arXiv See: \ud83e\udd17 \u2705 multilingual-e5-base 278M XLM-RoBERTa Base See: arXiv See: \ud83e\udd17 \u2705 multilingual-e5-large 560M XLM-RoBERTa Large See: arXiv See: \ud83e\udd17 \u2705 Deprecated Models Model #params Base/Student Model Teacher Model Train Dataset Supervised SimCSE-IndoBERT Lite Base 12M IndoBERT Lite Base N/A Wikipedia SimCSE-IndoRoBERTa Base 125M IndoRoBERTa Base N/A Wikipedia S-IndoBERT Base mMARCO 125M IndoBERT Base N/A mMARCO \u2705 all-IndoBERT Base p2 125M IndoBERT Base p2 N/A See: README \u2705"},{"location":"#results","title":"Results","text":""},{"location":"#semantic-textual-similarity_1","title":"Semantic Textual Similarity","text":""},{"location":"#machine-translated-indonesian-sts-b","title":"Machine Translated Indonesian STS-B","text":"Model Spearman's Correlation (%) \u2191 SimCSE-IndoBERT Base 70.13 ConGen-IndoBERT Lite Base 79.97 ConGen-IndoBERT Base 80.47 ConGen-SimCSE-IndoBERT Base 81.16 ConGen-Indo-e5 Small 80.94 SCT-IndoBERT Base 74.56 all-IndoBERT Base 73.84 all-IndoBERT Base-v2 76.03 all-IndoBERT Base-v4 75.99 all-NusaBERT Base-v4 77.65 all-NusaBERT Large-v4 79.23 all-Indo-e5 Small-v2 79.57 all-Indo-e5 Small-v3 79.95 all-Indo-e5 Small-v4 79.85 distiluse-base-multilingual-cased-v2 75.08 paraphrase-multilingual-mpnet-base-v2 83.83 multilingual-e5-small 78.89 multilingual-e5-base 79.72 multilingual-e5-large 79.44"},{"location":"#semrel2024-semantic-textual-relatedness-str","title":"SemRel2024: Semantic Textual Relatedness (STR)","text":"Model <code>dev</code> Spearman's Correlation (%) \u2191 <code>test</code> Spearman's Correlation (%) \u2191 SimCSE-IndoBERT Base 30.64 36.77 ConGen-IndoBERT Lite Base 35.95 41.73 ConGen-IndoBERT Base 35.05 39.14 ConGen-SimCSE-IndoBERT Base 33.71 37.73 ConGen-Indo-e5 Small 36.35 42.47 SCT-IndoBERT Base 41.50 43.25 all-IndoBERT Base 42.87 38.78 all-IndoBERT Base-v2 41.68 40.42 all-IndoBERT Base-v4 41.38 38.05 all-NusaBERT Base-v4 42.11 41.55 all-NusaBERT Large-v4 40.21 42.25 all-Indo-e5 Small-v2 39.79 43.85 all-Indo-e5 Small-v3 40.25 42.60 all-Indo-e5 Small-v4 40.20 42.90 distiluse-base-multilingual-cased-v2 37.22 49.35 paraphrase-multilingual-mpnet-base-v2 34.56 37.51 multilingual-e5-small 41.92 49.60 multilingual-e5-base 41.29 45.04 multilingual-e5-large 39.20 45.04"},{"location":"#retrieval_1","title":"Retrieval","text":""},{"location":"#miracl","title":"MIRACL","text":"Model R@1 (%) \u2191 MRR@10 (%) \u2191 nDCG@10 (%) \u2191 SimCSE-IndoBERT Base 36.04 48.25 39.70 ConGen-IndoBERT Lite Base 46.04 59.06 51.01 ConGen-IndoBERT Base 45.93 58.58 49.95 ConGen-SimCSE-IndoBERT Base 45.83 58.27 49.91 ConGen-Indo-e5 Small 55.00 66.74 58.95 SCT-IndoBERT Base 40.41 47.29 40.68 all-IndoBERT Base 65.52 75.92 70.13 all-IndoBERT Base-v2 67.18 76.59 70.16 all-IndoBERT Base-v4 67.91 77.37 70.97 all-NusaBERT Base-v4 67.08 77.47 71.24 all-NusaBERT Large-v4 68.43 78.29 71.99 all-Indo-e5 Small-v2 68.33 78.33 73.04 all-Indo-e5 Small-v3 68.12 78.22 73.09 all-Indo-e5 Small-v4 68.33 78.41 73.23 distiluse-base-multilingual-cased-v2 41.35 54.93 48.79 paraphrase-multilingual-mpnet-base-v2 52.81 65.07 57.97 multilingual-e5-small 70.20 79.61 74.80 multilingual-e5-base 70.00 79.50 75.16 multilingual-e5-large 70.83 80.58 76.16"},{"location":"#tydiqa","title":"TyDiQA","text":"Model R@1 (%) \u2191 MRR@10 (%) \u2191 nDCG@10 (%) \u2191 SimCSE-IndoBERT Base 61.94 69.89 73.52 ConGen-IndoBERT Lite Base 75.22 81.55 84.13 ConGen-IndoBERT Base 73.09 80.32 83.29 ConGen-SimCSE-IndoBERT Base 72.38 79.37 82.51 ConGen-Indo-e5 Small 84.60 89.30 91.27 SCT-IndoBERT Base 76.81 83.16 85.87 all-IndoBERT Base 88.14 91.47 92.91 all-IndoBERT Base-v2 87.61 90.91 92.31 all-IndoBERT Base-v4 89.02 92.59 93.91 all-NusaBERT Base-v4 92.74 94.95 95.73 all-NusaBERT Large-v4 93.62 95.77 96.56 all-Indo-e5 Small-v2 93.27 95.63 96.46 all-Indo-e5 Small-v3 93.27 95.72 96.58 all-Indo-e5 Small-v4 93.45 95.66 96.43 distiluse-base-multilingual-cased-v2 70.44 77.94 81.56 paraphrase-multilingual-mpnet-base-v2 81.41 87.05 89.44 multilingual-e5-small 91.50 94.34 95.39 multilingual-e5-base 93.45 95.88 96.69 multilingual-e5-large 94.69 96.71 97.44"},{"location":"#classification_1","title":"Classification","text":""},{"location":"#mteb-massive-intent-classification-id","title":"MTEB - Massive Intent Classification <code>(id)</code>","text":"Model Accuracy (%) \u2191 F1 Macro (%) \u2191 SimCSE-IndoBERT Base 59.71 57.70 ConGen-IndoBERT Lite Base 62.41 60.94 ConGen-IndoBERT Base 61.14 60.02 ConGen-SimCSE-IndoBERT Base 60.93 59.50 ConGen-Indo-e5 Small 62.92 60.18 SCT-IndoBERT Base 55.66 54.48 all-IndoBERT Base 58.40 57.21 all-IndoBERT Base-v2 58.31 57.11 all-IndoBERT Base-v4 57.80 56.71 all-NusaBERT Base-v4 62.10 60.38 all-NusaBERT Large-v4 61.41 59.93 all-Indo-e5 Small-v2 61.51 59.24 all-Indo-e5 Small-v3 61.63 59.29 all-Indo-e5 Small-v4 61.38 59.07 distiluse-base-multilingual-cased-v2 55.99 52.44 paraphrase-multilingual-mpnet-base-v2 65.43 63.55 multilingual-e5-small 64.16 61.33 multilingual-e5-base 66.63 63.88 multilingual-e5-large 70.04 67.66"},{"location":"#mteb-massive-scenario-classification-id","title":"MTEB - Massive Scenario Classification <code>(id)</code>","text":"Model Accuracy (%) \u2191 F1 Macro (%) \u2191 SimCSE-IndoBERT Base 66.14 65.56 ConGen-IndoBERT Lite Base 67.25 66.53 ConGen-IndoBERT Base 67.72 67.32 ConGen-SimCSE-IndoBERT Base 67.12 66.64 ConGen-Indo-e5 Small 66.92 66.29 SCT-IndoBERT Base 61.89 60.97 all-IndoBERT Base 66.37 66.31 all-IndoBERT Base-v2 66.02 65.97 all-IndoBERT Base-v4 66.33 66.14 all-NusaBERT Base-v4 70.17 70.18 all-NusaBERT Large-v4 70.10 70.38 all-Indo-e5 Small-v2 67.02 66.86 all-Indo-e5 Small-v3 67.27 67.13 all-Indo-e5 Small-v4 67.33 67.24 distiluse-base-multilingual-cased-v2 65.25 63.45 paraphrase-multilingual-mpnet-base-v2 70.72 70.58 multilingual-e5-small 67.92 67.23 multilingual-e5-base 70.70 70.26 multilingual-e5-large 74.11 73.82"},{"location":"#indonlu-emotion-classification-emot","title":"IndoNLU - Emotion Classification (EmoT)","text":"Model Accuracy (%) \u2191 F1 Macro (%) \u2191 SimCSE-IndoBERT Base 55.45 55.78 ConGen-IndoBERT Lite Base 58.18 58.84 ConGen-IndoBERT Base 57.04 57.06 ConGen-SimCSE-IndoBERT Base 59.54 60.37 ConGen-Indo-e5 Small 60.00 60.52 SCT-IndoBERT Base 61.13 61.70 all-IndoBERT Base 57.27 57.47 all-IndoBERT Base-v2 58.86 59.31 all-IndoBERT Base-v4 61.36 61.81 all-NusaBERT Base-v4 53.18 53.01 all-NusaBERT Large-v4 63.18 63.17 all-Indo-e5 Small-v2 58.18 57.99 all-Indo-e5 Small-v3 56.81 56.46 all-Indo-e5 Small-v4 56.94 57.04 distiluse-base-multilingual-cased-v2 63.63 64.13 paraphrase-multilingual-mpnet-base-v2 63.18 63.78 multilingual-e5-small 64.54 65.04 multilingual-e5-base 68.63 69.07 multilingual-e5-large 74.77 74.66"},{"location":"#indonlu-sentiment-analysis-smsa","title":"IndoNLU - Sentiment Analysis (SmSA)","text":"Model Accuracy (%) \u2191 F1 Macro (%) \u2191 SimCSE-IndoBERT Base 85.6 81.50 ConGen-IndoBERT Lite Base 81.2 75.59 ConGen-IndoBERT Base 85.4 82.12 ConGen-SimCSE-IndoBERT Base 83.0 78.74 ConGen-Indo-e5 Small 84.2 80.21 SCT-IndoBERT Base 82.0 76.92 all-IndoBERT Base 84.4 79.79 all-IndoBERT Base-v2 83.4 79.04 all-IndoBERT Base-v4 82.4 77.82 all-NusaBERT Base-v4 84.2 78.68 all-NusaBERT Large-v4 84.8 81.01 all-Indo-e5 Small-v2 82.0 78.15 all-Indo-e5 Small-v3 82.6 78.98 all-Indo-e5 Small-v4 82.6 79.14 distiluse-base-multilingual-cased-v2 78.8 73.64 paraphrase-multilingual-mpnet-base-v2 89.6 86.56 multilingual-e5-small 83.6 79.51 multilingual-e5-base 89.4 86.22 multilingual-e5-large 90.0 86.50"},{"location":"#pair-classification_1","title":"Pair Classification","text":""},{"location":"#indonli","title":"IndoNLI","text":"Model <code>test_lay</code> AP (%) \u2191 <code>test_expert</code> AP (%) \u2191 SimCSE-IndoBERT Base 56.06 50.72 ConGen-IndoBERT Lite Base 69.44 53.74 ConGen-IndoBERT Base 71.14 56.35 ConGen-SimCSE-IndoBERT Base 70.80 56.59 ConGen-Indo-e5 Small 70.51 55.67 SCT-IndoBERT Base 59.82 53.41 all-IndoBERT Base 72.01 56.79 all-IndoBERT Base-v2 71.36 56.83 all-IndoBERT Base-v4 70.99 58.99 all-NusaBERT Base-v4 73.07 59.86 all-NusaBERT Large-v4 73.26 61.14 all-Indo-e5 Small-v2 76.29 57.05 all-Indo-e5 Small-v3 75.21 56.62 all-Indo-e5 Small-v4 75.05 57.42 distiluse-base-multilingual-cased-v2 58.48 50.50 paraphrase-multilingual-mpnet-base-v2 74.87 57.96 multilingual-e5-small 63.97 51.85 multilingual-e5-base 60.25 50.91 multilingual-e5-large 61.39 51.62"},{"location":"#credits","title":"Credits","text":"<p>Indonesian Sentence Embeddings is developed with love by:</p>"},{"location":"evaluation/evaluation/","title":"Evaluation","text":""},{"location":"evaluation/evaluation/#machine-translated-sts-b","title":"Machine Translated STS-B","text":"<p>Inspired by Thai Sentence Vector Benchmark, we translated the STS-B dev and test set to Indonesian via Google Translate API. This dataset will be used to evaluate our model's Spearman correlation score on the translated test set. You can find the translated dataset on \ud83e\udd17 HuggingFace Hub.</p> <p>For practical purposes, we used Sentence Transformer's <code>EmbeddingSimilarityEvaluator</code> to perform inference and evaluate our models.</p>"},{"location":"evaluation/evaluation/#example","title":"Example","text":"<pre><code>python sts/eval_sts.py \\\n    --model-name LazarusNLP/congen-indobert-base \\\n    --test-dataset-name LazarusNLP/stsb_mt_id \\\n    --test-dataset-split test \\\n    --test-text-column-1 text_1 \\\n    --test-text-column-2 text_2 \\\n    --test-label-column correlation \\\n    --test-batch-size 32\n</code></pre>"},{"location":"evaluation/evaluation/#semrel2024-semantic-textual-relatedness-str","title":"SemRel2024: Semantic Textual Relatedness (STR)","text":"<p>SemRel2024 is a collection of Semantic Textual Relatedness (STR) datasets for 14 languages, including African and Asian languages. The datasets are composed of sentence pairs, each assigned a relatedness score between 0 (completely) unrelated and 1 (maximally related) with a large range of expected relatedness values. SemRel2024 dataset was used as part of the SemEval2024 shared task 1. The task aims to evaluate the ability of systems to measure the semantic relatedness between two sentences.</p> <p>We used Sentence Transformer's <code>EmbeddingSimilarityEvaluator</code> to perform inference and evaluate our models.</p>"},{"location":"evaluation/evaluation/#example_1","title":"Example","text":"<pre><code>python sts/eval_sts.py \\\n    --model-name LazarusNLP/congen-indobert-base \\\n    --test-dataset-name SemRel/SemRel2024 \\\n    --test-dataset-config ind \\\n    --test-dataset-split test \\\n    --test-text-column-1 sentence1 \\\n    --test-text-column-2 sentence2 \\\n    --test-label-column label \\\n    --test-batch-size 32\n</code></pre>"},{"location":"evaluation/evaluation/#miracl-multilingual-information-retrieval-across-a-continuum-of-languages","title":"MIRACL (Multilingual Information Retrieval Across a Continuum of Languages)","text":"<p>MIRACL (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval dataset that focuses on search across 18 different languages, which collectively encompass over three billion native speakers around the world. We evaluated our models on the Indonesian subset of MIRACL.</p> <p>We used Sentence Transformer's <code>InformationRetrievalEvaluator</code> to perform inference and evaluate our models.</p>"},{"location":"evaluation/evaluation/#example_2","title":"Example","text":"<pre><code>python retrieval/eval_miracl.py \\\n    --model-name LazarusNLP/congen-simcse-indobert-base \\\n    --test-dataset-name miracl/miracl \\\n    --test-dataset-config id \\\n    --test-dataset-split dev \\\n    --test-batch-size 32 \\\n    --output-folder retrieval/results/congen-simcse-indobert-base\n</code></pre>"},{"location":"evaluation/evaluation/#tydiqa","title":"TyDiQA","text":"<p>TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. We evaluated our models on the Indonesian subset of MIRACL.</p> <p>We used Sentence Transformer's <code>InformationRetrievalEvaluator</code> to perform inference and evaluate our models.</p>"},{"location":"evaluation/evaluation/#example_3","title":"Example","text":"<pre><code>python retrieval/eval_tydiqa.py \\\n    --model-name LazarusNLP/congen-simcse-indobert-base \\\n    --test-dataset-name khalidalt/tydiqa-goldp \\\n    --test-dataset-config indonesian \\\n    --test-dataset-split validation \\\n    --test-batch-size 32 \\\n    --output-folder retrieval/results/congen-simcse-indobert-base\n</code></pre>"},{"location":"evaluation/evaluation/#massive-text-embedding-benchmark-mteb","title":"Massive Text Embedding Benchmark (MTEB)","text":"<p>The Massive Text Embedding Benchmark (MTEB) aims to provide clarity on how models perform on a variety of embedding tasks and thus serves as the gateway to finding universal text embeddings applicable to a variety of tasks. We evaluated our models on Indonesian subsets of MTEB that consists of two classification subsets: <code>MassiveIntentClassification (id)</code> and <code>MassiveScenarioClassification (id)</code>.</p> <p>We used the official MTEB code to automatically evaluate our models and ensure fairness across evaluations. Follow the official installation instructions found in the MTEB repository.</p>"},{"location":"evaluation/evaluation/#example_4","title":"Example","text":"<pre><code>mteb \\\n    -m LazarusNLP/congen-simcse-indobert-base \\\n    -l id  \\\n    --output_folder mteb/results/congen-simcse-indobert-base\n</code></pre>"},{"location":"evaluation/evaluation/#indonlu-text-classification","title":"IndoNLU Text Classification","text":"<p>Normally, sentence embedding models are leveraged for downstream tasks such as information retrieval, semantic search, clustering, etc. Text classification could similarly leverage these models' sentence embedding capabilities.</p> <p>We will be doing emotion classification and sentiment analysis on the EmoT and SmSA subsets of IndoNLU, respectively. To do so, we will be doing the same approach as Thai Sentence Vector Benchmark and simply fit a Linear SVC on sentence representations of our texts with their corresponding labels. Thus, unlike conventional fine-tuning method where the backbone model is also updated, the Sentence Transformer stays frozen in our case; with only the classification head being trained.</p>"},{"location":"evaluation/evaluation/#example-emotion-classification","title":"Example: Emotion Classification","text":"<pre><code>python classification/eval_classification.py \\\n    --model-name LazarusNLP/simcse-indobert-base \\\n    --dataset-name indonlp/indonlu \\\n    --dataset-config emot \\\n    --train-split-name train \\\n    --test-split-name test \\\n    --text-column tweet \\\n    --label-column label\n</code></pre>"},{"location":"evaluation/evaluation/#example-sentiment-analysis","title":"Example: Sentiment Analysis","text":"<pre><code>python classification/eval_classification.py \\\n    --model-name LazarusNLP/simcse-indobert-base \\\n    --dataset-name indonlp/indonlu \\\n    --dataset-config smsa \\\n    --train-split-name train \\\n    --test-split-name test \\\n    --text-column text \\\n    --label-column label\n</code></pre>"},{"location":"evaluation/evaluation/#indonli-pair-classification","title":"IndoNLI Pair Classification","text":"<p>We can similarly leverage sentence embedding models for zero-shot pair classification tasks. In our case, we will be doing zero-shot natural language inference on the IndoNLI dataset. We follow the same evaluation procedure as done on the MTEB PairClassification Benchmark. We drop all neutral pairs and re-mapped contradictions as <code>0</code>s and entailments as <code>1</code>s. Afterwards, we will search for the best threshold values and find the maximum average precision (AP) score, which also serves as the evaluation metric.</p>"},{"location":"evaluation/evaluation/#example_5","title":"Example","text":"<pre><code>for split in test_lay test_expert\ndo\n  python pair_classification/eval_pair_classification.py \\\n      --model-name LazarusNLP/simcse-indobert-base \\\n      --dataset-name indonli \\\n      --test-split-name $split \\\n      --text-column-1 premise \\\n      --text-column-2 hypothesis \\\n      --label-column label\ndone\n</code></pre>"},{"location":"evaluation/evaluation/#references","title":"References","text":"<pre><code>@misc{Thai-Sentence-Vector-Benchmark-2022,\n  author = {Limkonchotiwat, Peerat},\n  title = {Thai-Sentence-Vector-Benchmark},\n  year = {2022},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/mrpeerat/Thai-Sentence-Vector-Benchmark}}\n}\n</code></pre> <pre><code>@article{10.1162/tacl_a_00595,\n  author = {Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},\n  title = \"{MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages}\",\n  journal = {Transactions of the Association for Computational Linguistics},\n  volume = {11},\n  pages = {1114-1131},\n  year = {2023},\n  month = {09},\n  issn = {2307-387X},\n  doi = {10.1162/tacl_a_00595},\n  url = {https://doi.org/10.1162/tacl\\_a\\_00595},\n  eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\_a\\_00595/2157340/tacl\\_a\\_00595.pdf},\n}\n</code></pre> <pre><code>@article{muennighoff2022mteb,\n  doi = {10.48550/ARXIV.2210.07316},\n  url = {https://arxiv.org/abs/2210.07316},\n  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\\\"\\i}c and Reimers, Nils},\n  title = {MTEB: Massive Text Embedding Benchmark},\n  publisher = {arXiv},\n  journal={arXiv preprint arXiv:2210.07316},  \n  year = {2022}\n}\n</code></pre> <pre><code>@inproceedings{wilie2020indonlu,\n  title={IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding},\n  author={Bryan Wilie and Karissa Vincentio and Genta Indra Winata and Samuel Cahyawijaya and X. Li and Zhi Yuan Lim and S. Soleman and R. Mahendra and Pascale Fung and Syafri Bahar and A. Purwarianti},\n  booktitle={Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},\n  year={2020}\n}\n</code></pre> <pre><code>@inproceedings{mahendra-etal-2021-indonli,\n    itle = \"{I}ndo{NLI}: A Natural Language Inference Dataset for {I}ndonesian\",\n  author = \"Mahendra, Rahmad and Aji, Alham Fikri and Louvan, Samuel and Rahman, Fahrurrozi and Vania, Clara\",\n  booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n  month = nov,\n  year = \"2021\",\n  address = \"Online and Punta Cana, Dominican Republic\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://aclanthology.org/2021.emnlp-main.821\",\n  pages = \"10511--10527\",\n}\n</code></pre> <pre><code>@inproceedings{reimers-2019-sentence-bert,\n  title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n  author = \"Reimers, Nils and Gurevych, Iryna\",\n  booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n  month = \"11\",\n  year = \"2019\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"training/all/","title":"All Supervised Datasets","text":"<p>Inspired by all-MiniLM-L6-v2, we fine-tuned Indonesian sentence embedding models on a set of existing supervised datasets. The tasks included in the training dataset are: question-answering, textual entailment, retrieval, commonsense reasoning, and natural language inference. Currently, our script simply concatenates these datasets and our models are trained conventionally using the <code>MultipleNegativesRankingLoss</code>.</p>"},{"location":"training/all/#training-data","title":"Training Data","text":"Dataset Task Data Instance Number of Training Tuples indonli Natural Language Inference <code>(premise, entailment, contradiction)</code> 3,914 indolem/indo_story_cloze Commonsense Reasoning <code>(context, correct ending, incorrect ending)</code> 1,000 unicamp-dl/mmarco Passage Retrieval <code>(query, positive passage, negative passage)</code> 100,000 miracl/miracl Passage Retrieval <code>(query, positive passage, negative passage)</code> 8,086 SEACrowd/wrete Textual Entailment <code>(sentenceA, sentenceB)</code> 183 SEACrowd/indolem_ntp Textual Entailment <code>(tweet, next tweet)</code> 5,681 khalidalt/tydiqa-goldp Extractive Question-Answering <code>(question, passage)</code>, <code>(question, answer)</code> 11,404 SEACrowd/facqa Extractive Question-Answering <code>(question, passage)</code>, <code>(question, answer)</code> 4,990 included in v2 indonesian-nlp/lfqa_id Open-domain Question-Answering <code>(question, answer)</code> 226,147 jakartaresearch/indoqa Extractive Question-Answering <code>(question, passage)</code>, <code>(question, answer)</code> 6,498 jakartaresearch/id-paraphrase-detection Paraphrase <code>(sentence, rephrased sentence)</code> 4,076 included in v3 LazarusNLP/multilingual-NLI-26lang-2mil7-id Natural Language Inference <code>(premise, entailment, hypothesis)</code> 41,924 included in v4 nthakur/swim-ir-monolingual Passage Retrieval <code>(query, positive passage, negative passage)</code> 227,145 Total 641,048"},{"location":"training/all/#all-supervised-datasets-with-multiplenegativesrankingloss","title":"All Supervised Datasets with MultipleNegativesRankingLoss","text":""},{"location":"training/all/#indobert-base","title":"IndoBERT Base","text":"<pre><code>python train_all_mnrl.py \\\n    --model-name indobenchmark/indobert-base-p1 \\\n    --max-seq-length 128 \\\n    --num-epochs 5 \\\n    --train-batch-size-pairs 384 \\\n    --train-batch-size-triplets 256 \\\n    --learning-rate 2e-5\n</code></pre>"},{"location":"training/all/#multilingual-e5-small","title":"Multilingual e5 Small","text":"<pre><code>python train_all_mnrl.py \\\n    --model-name intfloat/multilingual-e5-small \\\n    --max-seq-length 128 \\\n    --num-epochs 5 \\\n    --train-batch-size-pairs 384 \\\n    --train-batch-size-triplets 256 \\\n    --learning-rate 2e-5\n</code></pre>"},{"location":"training/all/#all-supervised-datasets-with-cachedmultiplenegativesrankingloss","title":"All Supervised Datasets with CachedMultipleNegativesRankingLoss","text":""},{"location":"training/all/#indobert-base_1","title":"IndoBERT Base","text":"<pre><code>python train_all_cached_mnrl.py \\\n    --model-name indobenchmark/indobert-base-p1 \\\n    --max-seq-length 128 \\\n    --num-epochs 5 \\\n    --train-batch-size-pairs 384 \\\n    --train-batch-size-triplets 256 \\\n    --mini-batch-size 320 \\\n    --learning-rate 2e-5\n</code></pre>"},{"location":"training/all/#nusabert-large","title":"NusaBERT Large","text":"<pre><code>python train_all_cached_mnrl.py \\\n    --model-name LazarusNLP/NusaBERT-large \\\n    --max-seq-length 128 \\\n    --num-epochs 3 \\\n    --train-batch-size-pairs 384 \\\n    --train-batch-size-triplets 256 \\\n    --mini-batch-size 80 \\\n    --learning-rate 2e-5\n</code></pre>"},{"location":"training/all/#references","title":"References","text":"<pre><code>@inproceedings{mahendra-etal-2021-indonli,\n  title=\"{I}ndo{NLI}: A Natural Language Inference Dataset for {I}ndonesian\",\n  author=\"Mahendra, Rahmad and Aji, Alham Fikri and Louvan, Samuel and Rahman, Fahrurrozi and Vania, Clara\",\n  booktitle=\"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n  month=nov,\n  year=\"2021\",\n  address=\"Online and Punta Cana, Dominican Republic\",\n  publisher=\"Association for Computational Linguistics\",\n  url=\"https://aclanthology.org/2021.emnlp-main.821\",\n  pages=\"10511--10527\",\n}\n</code></pre> <pre><code>@inproceedings{koto2022cloze,\n  title={Cloze evaluation for deeper understanding of commonsense stories in Indonesian},\n  author={Koto, Fajri and Baldwin, Timothy and Lau, Jey Han},\n  booktitle={Proceedings of the First Workshop on Commonsense Representation and Reasoning (CSRR 2022)},\n  pages={8--16},\n  year={2022}\n}\n</code></pre> <pre><code>@misc{bonifacio2021mmarco,\n  title={mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset}, \n  author={Luiz Henrique Bonifacio and Vitor Jeronymo and Hugo Queiroz Abonizio and Israel Campiotti and Marzieh Fadaee and  and Roberto Lotufo and Rodrigo Nogueira},\n  year={2021},\n  eprint={2108.13897},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n</code></pre> <pre><code>@article{10.1162/tacl_a_00595,\n  author={Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},\n  title=\"{MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages}\",\n  journal={Transactions of the Association for Computational Linguistics},\n  volume={11},\n  pages={1114-1131},\n  year={2023},\n  month={09},\n  issn={2307-387X},\n  doi={10.1162/tacl_a_00595},\n  url={https://doi.org/10.1162/tacl\\_a\\_00595},\n  eprint={https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\_a\\_00595/2157340/tacl\\_a\\_00595.pdf},\n}\n</code></pre> <pre><code>@inproceedings{wilie2020indonlu,\n  title={IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding},\n  author={Wilie, Bryan and Vincentio, Karissa and Winata, Genta Indra and Cahyawijaya, Samuel and Li, Xiaohong and Lim, Zhi Yuan and Soleman, Sidik and Mahendra, Rahmad and Fung, Pascale and Bahar, Syafri and others},\n  booktitle={Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},\n  pages={843--857},\n  year={2020}\n}\n</code></pre> <pre><code>@article{DBLP:journals/corr/abs-2011-00677,\n  author    = {Fajri Koto and\n               Afshin Rahimi and\n               Jey Han Lau and\n               Timothy Baldwin},\n  title     = {IndoLEM and IndoBERT: {A} Benchmark Dataset and Pre-trained Language\n               Model for Indonesian {NLP}},\n  journal   = {CoRR},\n  volume    = {abs/2011.00677},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2011.00677},\n  eprinttype = {arXiv},\n  eprint    = {2011.00677},\n  timestamp = {Fri, 06 Nov 2020 15:32:47 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-00677.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> <pre><code>@inproceedings{ruder-etal-2021-xtreme,\n  title = \"{XTREME}-{R}: Towards More Challenging and Nuanced Multilingual Evaluation\",\n  author = \"Ruder, Sebastian  and\n    Constant, Noah  and\n    Botha, Jan  and\n    Siddhant, Aditya  and\n    Firat, Orhan  and\n    Fu, Jinlan  and\n    Liu, Pengfei  and\n    Hu, Junjie  and\n    Garrette, Dan  and\n    Neubig, Graham  and\n    Johnson, Melvin\",\n  booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n  month = nov,\n  year = \"2021\",\n  address = \"Online and Punta Cana, Dominican Republic\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://aclanthology.org/2021.emnlp-main.802\",\n  doi = \"10.18653/v1/2021.emnlp-main.802\",\n  pages = \"10215--10245\",\n}\n</code></pre> <pre><code>@inproceedings{purwarianti2007machine,\n  title={A Machine Learning Approach for Indonesian Question Answering System},\n  author={Ayu Purwarianti, Masatoshi Tsuchiya, and Seiichi Nakagawa},\n  booktitle={Proceedings of Artificial Intelligence and Applications },\n  pages={573--578},\n  year={2007}\n}\n</code></pre>"},{"location":"training/mMARCO/","title":"mMARCO","text":"<p>mMARCO is a multilingual version of the MS MARCO passage ranking dataset, translated via Google Translate API. It supports up to 14 languages, including Indonesian. </p> <p>Unlike the original MS MARCO dataset, this version only has query-positive-negative triplets. In the original version, for instance, it had a list of passages which may be relevant to the query, and a label for the most relevant passage.</p>"},{"location":"training/mMARCO/#bi-encoder-with-multiplenegativesrankingloss","title":"Bi-Encoder with MultipleNegativesRankingLoss","text":""},{"location":"training/mMARCO/#indobert-base","title":"IndoBERT Base","text":"<pre><code>python train_bi-encoder_mmarco_mnrl.py \\\n    --model-name indobenchmark/indobert-base-p1 \\\n    --train-dataset-name unicamp-dl/mmarco \\\n    --train-dataset-config indonesian \\\n    --max-seq-length 32 \\\n    --max-train-samples 1000000 \\\n    --num-epochs 5 \\\n    --train-batch-size 128 \\\n    --learning-rate 2e-5 \\\n    --warmup-ratio 0.1\n</code></pre>"},{"location":"training/mMARCO/#references","title":"References","text":"<pre><code>@misc{bonifacio2021mmarco,\n  title={mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset}, \n  author={Luiz Henrique Bonifacio and Vitor Jeronymo and Hugo Queiroz Abonizio and Israel Campiotti and Marzieh Fadaee and  and Roberto Lotufo and Rodrigo Nogueira},\n  year={2021},\n  eprint={2108.13897},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"unsupervised_learning/ConGen/","title":"ConGen","text":"<p>ConGen is an unsupervised, knowledge distillation technique that aims to control and generalize smaller student model from a larger sentence embedding teacher model. In short, the technique enforces the student model to mimic the logits of the teacher model on an instance queue subset of the training data (control) and also generalize it to augmentations of texts for robustness.</p> <p>Training via ConGen requires an unsupervised corpus, which is readily available for Indonesian texts. In our experiments, we used Wikipedia texts. As for the data augmentation method, Limkonchotiwat et al. (2022) proposed using back-translation via an NMT model or Google Translate API. However, since that is costly to compute for 1 million texts, we opted for a simple single-word deletion technique.</p>"},{"location":"unsupervised_learning/ConGen/#congen-with-single-word-deletion","title":"ConGen with Single-word Deletion","text":""},{"location":"unsupervised_learning/ConGen/#indobert-base","title":"IndoBERT Base","text":"<pre><code>python train_con_gen.py \\\n    --model-name indobenchmark/indobert-base-p1 \\\n    --train-dataset-name LazarusNLP/wikipedia_id_20230520 \\\n    --max-seq-length 32 \\\n    --max-train-samples 1000000 \\\n    --num-epochs 20 \\\n    --train-batch-size 128 \\\n    --early-stopping-patience 7 \\\n    --learning-rate 1e-4 \\\n    --teacher-model-name sentence-transformers/paraphrase-multilingual-mpnet-base-v2 \\\n    --queue-size 65536 \\\n    --student-temp 0.5 \\\n    --teacher-temp 0.5\n</code></pre>"},{"location":"unsupervised_learning/ConGen/#indobert-lite-base","title":"IndoBERT Lite Base","text":"<pre><code>python train_con_gen.py \\\n    --model-name indobenchmark/indobert-lite-base-p1 \\\n    --train-dataset-name LazarusNLP/wikipedia_id_20230520 \\\n    --max-seq-length 32 \\\n    --max-train-samples 1000000 \\\n    --num-epochs 20 \\\n    --train-batch-size 128 \\\n    --early-stopping-patience 7 \\\n    --learning-rate 3e-4 \\\n    --teacher-model-name sentence-transformers/paraphrase-multilingual-mpnet-base-v2 \\\n    --queue-size 65536 \\\n    --student-temp 0.05 \\\n    --teacher-temp 0.05\n</code></pre>"},{"location":"unsupervised_learning/ConGen/#simcse-indobert-base","title":"SimCSE-IndoBERT Base","text":"<pre><code>python train_con_gen.py \\\n    --model-name LazarusNLP/simcse-indobert-base \\\n    --train-dataset-name LazarusNLP/wikipedia_id_20230520 \\\n    --max-seq-length 32 \\\n    --max-train-samples 1000000 \\\n    --num-epochs 20 \\\n    --train-batch-size 128 \\\n    --early-stopping-patience 7 \\\n    --learning-rate 1e-4 \\\n    --teacher-model-name sentence-transformers/paraphrase-multilingual-mpnet-base-v2 \\\n    --queue-size 65536 \\\n    --student-temp 0.5 \\\n    --teacher-temp 0.5\n</code></pre>"},{"location":"unsupervised_learning/ConGen/#references","title":"References","text":"<pre><code>@inproceedings{limkonchotiwat-etal-2022-congen,\n  title = \"{ConGen}: Unsupervised Control and Generalization Distillation For Sentence Representation\",\n  author = \"Limkonchotiwat, Peerat  and\n    Ponwitayarat, Wuttikorn  and\n    Lowphansirikul, Lalita and\n    Udomcharoenchaikit, Can  and\n    Chuangsuwanich, Ekapol  and\n    Nutanong, Sarana\",\n  booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2022\",\n  year = \"2022\",\n  publisher = \"Association for Computational Linguistics\",\n}\n</code></pre>"},{"location":"unsupervised_learning/SCT/","title":"SCT","text":"<p>SCT is an Efficient Self-Supervised Cross-View Training For Sentence Embedding, that also supports knowledge-distillation from a fine-tuned sentence embedding teacher model. Like ConGen, the technique enforces the student model to mimic the logits of the teacher model on an instance queue and also generalize it to augmentations of texts for robustness. Unlike ConGen, the instance queue is generated by random (fake) sentence embeddings instead of actual sentence vectors.</p> <p>Training via SCT requires an unsupervised corpus, which is readily available for Indonesian texts. In our experiments, we used Wikipedia texts. As for the data augmentation method, Limkonchotiwat et al. (2023) proposed using back-translation via an NMT model or Google Translate API. However, since that is costly to compute for 1 million texts, we opted for a simple single-word deletion technique. Interestingly, we found out that using a backtranslated corpus resulted in a poorer model than using single-word deletion. We hypothesize that this is due to the quality of open-source Indonesian machine translation models. Further study is required.</p>"},{"location":"unsupervised_learning/SCT/#sct-distillation-with-single-word-deletion","title":"SCT Distillation with Single-word Deletion","text":""},{"location":"unsupervised_learning/SCT/#indobert-base","title":"IndoBERT Base","text":"<pre><code>python train_sct_distillation.py \\\n    --model-name indobenchmark/indobert-base-p1 \\\n    --train-dataset-name LazarusNLP/wikipedia_id_backtranslated \\\n    --train_text_column_1 text \\\n    --do_corrupt \\\n    --max-seq-length 128 \\\n    --num-epochs 20 \\\n    --train-batch-size 128 \\\n    --early-stopping-patience 7 \\\n    --learning-rate 1e-4 \\\n    --teacher-model-name sentence-transformers/paraphrase-multilingual-mpnet-base-v2 \\\n    --queue-size 65536 \\\n    --student-temp 0.5 \\\n    --teacher-temp 0.5\n</code></pre>"},{"location":"unsupervised_learning/SCT/#sct-distillation-with-back-translated-corpus","title":"SCT Distillation with Back-translated Corpus","text":""},{"location":"unsupervised_learning/SCT/#indobert-base_1","title":"IndoBERT Base","text":"<pre><code>python train_sct_distillation.py \\\n    --model-name indobenchmark/indobert-base-p1 \\\n    --train-dataset-name LazarusNLP/wikipedia_id_backtranslated \\\n    --train_text_column_1 text \\\n    --train_text_column_2 text_bt \\\n    --max-seq-length 128 \\\n    --num-epochs 20 \\\n    --train-batch-size 128 \\\n    --early-stopping-patience 7 \\\n    --learning-rate 1e-4 \\\n    --teacher-model-name sentence-transformers/paraphrase-multilingual-mpnet-base-v2 \\\n    --queue-size 65536 \\\n    --student-temp 0.5 \\\n    --teacher-temp 0.5\n</code></pre>"},{"location":"unsupervised_learning/SCT/#references","title":"References","text":"<pre><code>@article{10.1162/tacl_a_00620,\n    author = {Limkonchotiwat, Peerat and Ponwitayarat, Wuttikorn and Lowphansirikul, Lalita and Udomcharoenchaikit, Can and Chuangsuwanich, Ekapol and Nutanong, Sarana},\n    title = \"{An Efficient Self-Supervised Cross-View Training For Sentence Embedding}\",\n    journal = {Transactions of the Association for Computational Linguistics},\n    volume = {11},\n    pages = {1572-1587},\n    year = {2023},\n    month = {12},\n    issn = {2307-387X},\n    doi = {10.1162/tacl_a_00620},\n    url = {https://doi.org/10.1162/tacl\\_a\\_00620},\n    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\_a\\_00620/2196817/tacl\\_a\\_00620.pdf},\n}\n</code></pre>"},{"location":"unsupervised_learning/SimCSE/","title":"SimCSE","text":"<p>Unsupervised SimCSE is a contrastive learning framework that proposes the usage of different dropout masks as means to generate augmented representations of the same text. There is also a supervised variant of SimCSE that leverages annoated pairs from NLI datasets, using the same contrastive learning framework.</p> <p>Training via SimCSE requires an unsupervised corpus, which is readily available for Indonesian texts. In our experiments, we used Wikipedia texts. We used the Sentence Transformer implementation of SimCSE.</p>"},{"location":"unsupervised_learning/SimCSE/#unsupervised-simcse-with-multiplenegativesrankingloss","title":"Unsupervised SimCSE with MultipleNegativesRankingLoss","text":""},{"location":"unsupervised_learning/SimCSE/#indobert-base","title":"IndoBERT Base","text":"<pre><code>python train_sim_cse.py \\\n    --model-name indobenchmark/indobert-base-p1 \\\n    --train-dataset-name LazarusNLP/wikipedia_id_20230520 \\\n    --max-train-samples 1000000 \\\n    --max-seq-length 32 \\\n    --num-epochs 1 \\\n    --train-batch-size 128 \\\n    --learning-rate 3e-5\n</code></pre>"},{"location":"unsupervised_learning/SimCSE/#indobert-lite-base","title":"IndoBERT Lite Base","text":"<pre><code>python train_sim_cse.py \\\n    --model-name indobenchmark/indobert-lite-base-p1 \\\n    --train-dataset-name LazarusNLP/wikipedia_id_20230520 \\\n    --max-train-samples 1000000 \\\n    --max-seq-length 75 \\\n    --num-epochs 1 \\\n    --train-batch-size 128 \\\n    --learning-rate 3e-5\n</code></pre>"},{"location":"unsupervised_learning/SimCSE/#indoroberta-base","title":"IndoRoBERTa Base","text":"<pre><code>python train_sim_cse.py \\\n    --model-name flax-community/indonesian-roberta-base \\\n    --train-dataset-name LazarusNLP/wikipedia_id_20230520 \\\n    --max-train-samples 1000000 \\\n    --max-seq-length 32 \\\n    --num-epochs 1 \\\n    --train-batch-size 128 \\\n    --learning-rate 3e-5\n</code></pre>"},{"location":"unsupervised_learning/SimCSE/#references","title":"References","text":"<pre><code>@inproceedings{gao2021simcse,\n  title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings},\n  author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},\n  booktitle={Empirical Methods in Natural Language Processing (EMNLP)},\n  year={2021}\n}\n</code></pre>"}]}